{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pybtex\n",
    "import pandas as pd\n",
    "import json\n",
    "import traceback\n",
    "from pybtex.database.input import bibtex\n",
    "\n",
    "bib_path = 'anthology+abstracts.bib'\n",
    "#open a bibtex file\n",
    "parser = bibtex.Parser()\n",
    "bibdata = parser.parse_file(bib_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment classification is an active research area with several applications including analysis of political opinions, classifying comments, movie reviews, news reviews and product reviews. To employ rule based sentiment classification, we require sentiment lexicons. However, manual construction of sentiment lexicon is time consuming and costly for resource-limited languages. To bypass manual development time and costs, we tried to build Amharic Sentiment Lexicons relying on corpus based approach. The intention of this approach is to handle sentiment terms specific to Amharic language from Amharic Corpus. Small set of seed terms are manually prepared from three parts of speech such as noun, adjective and verb. We developed algorithms for constructing Amharic sentiment lexicons automatically from Amharic news corpus. Corpus based approach is proposed relying on the word co-occurrence distributional embedding including frequency based embedding (i.e. Positive Point-wise Mutual Information PPMI). Using PPMI with threshold value of 100 and 200, we got corpus based Amharic Sentiment lexicons of size 1811 and 3794 respectively by expanding 519 seeds. Finally, the lexicon generated in corpus based approach is evaluated.\n",
      "{\"abstract_id\": 1, \"sentences\": [\"Sentiment classification is an active research area with several applications including analysis of political opinions, classifying comments, movie reviews, news reviews and product reviews\", \" To employ rule based sentiment classification, we require sentiment lexicons\", \" However, manual construction of sentiment lexicon is time consuming and costly for resource-limited languages\", \" To bypass manual development time and costs, we tried to build Amharic Sentiment Lexicons relying on corpus based approach\", \" The intention of this approach is to handle sentiment terms specific to Amharic language from Amharic Corpus\", \" Small set of seed terms are manually prepared from three parts of speech such as noun, adjective and verb\", \" We developed algorithms for constructing Amharic sentiment lexicons automatically from Amharic news corpus\", \" Corpus based approach is proposed relying on the word co-occurrence distributional embedding including frequency based embedding (i\", \"e\", \" Positive Point-wise Mutual Information PPMI)\", \" Using PPMI with threshold value of 100 and 200, we got corpus based Amharic Sentiment lexicons of size 1811 and 3794 respectively by expanding 519 seeds\", \" Finally, the lexicon generated in corpus based approach is evaluated\", \"\"], \"labels\": [\"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\"]}\n",
      "User generated content is bringing new aspects of processing data on the web. Due to the advancement of World Wide Web technology, users are not only consumer of web contents but also they are producers of contents in the form of text, audio, video and picture. This study focuses on the analysis of textual contents with subjective information (referring to sentiment analysis). Most of conventional approaches of sentiment analysis do not effectively capture negation in languages where there are limited computational linguistic resources (e.g. Amharic). For this research, we proposed Amharic negation handling framework for Amharic sentiment classification. The proposed framework combines the lexicon based sentiment classification approach and character ngram based machine learning algorithms. Finally, the performance of framework is evaluated using the annotated Amharic news comments. The system is performing the best of all models and the baselines with accuracy of 98.0. The result is compared with the baselines (without negation handling and word level ngram model).\n",
      "{\"abstract_id\": 2, \"sentences\": [\"User generated content is bringing new aspects of processing data on the web\", \" Due to the advancement of World Wide Web technology, users are not only consumer of web contents but also they are producers of contents in the form of text, audio, video and picture\", \" This study focuses on the analysis of textual contents with subjective information (referring to sentiment analysis)\", \" Most of conventional approaches of sentiment analysis do not effectively capture negation in languages where there are limited computational linguistic resources (e\", \"g\", \" Amharic)\", \" For this research, we proposed Amharic negation handling framework for Amharic sentiment classification\", \" The proposed framework combines the lexicon based sentiment classification approach and character ngram based machine learning algorithms\", \" Finally, the performance of framework is evaluated using the annotated Amharic news comments\", \" The system is performing the best of all models and the baselines with accuracy of 98\", \"0\", \" The result is compared with the baselines (without negation handling and word level ngram model)\", \"\"], \"labels\": [\"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\"]}\n",
      "The Web has become a source of information, where information is provided by humans for humans and its growth has increased necessity to get solutions that intelligently extract valuable knowledge from existing and newly added web documents with no (minimal) supervisions. However, due to the unstructured nature of existing data on the Web, effective extraction of this knowledge is limited for both human beings and software agents. Thus, this research work designed generic and embedding oriented framework that automatically annotates semantically Amharic web documents using ontology. This framework significantly reduces manual annotation and learning cost used for semantic annotation of Amharic web documents with its nature of adaptability with minimal modification. The results have also implied that neural network techniques are promising for semantic annotation, especially for less resourced languages like Amharic in comparison to language dependent techniques that have cost of speed and challenge of adaptation into new domains and languages. We experiment the feasibility of the proposed approach using Amharic news collected from WALTA news agency and Amharic Wikipedia. Our results show that the proposed solution exhibits 70.68{\\%} of precision, 66.89{\\%} of recall and 68.53{\\%} of f-measure in semantic annotation for a morphologically complex Amharic language with limited size dataset.\n",
      "{\"abstract_id\": 3, \"sentences\": [\"The Web has become a source of information, where information is provided by humans for humans and its growth has increased necessity to get solutions that intelligently extract valuable knowledge from existing and newly added web documents with no (minimal) supervisions\", \" However, due to the unstructured nature of existing data on the Web, effective extraction of this knowledge is limited for both human beings and software agents\", \" Thus, this research work designed generic and embedding oriented framework that automatically annotates semantically Amharic web documents using ontology\", \" This framework significantly reduces manual annotation and learning cost used for semantic annotation of Amharic web documents with its nature of adaptability with minimal modification\", \" The results have also implied that neural network techniques are promising for semantic annotation, especially for less resourced languages like Amharic in comparison to language dependent techniques that have cost of speed and challenge of adaptation into new domains and languages\", \" We experiment the feasibility of the proposed approach using Amharic news collected from WALTA news agency and Amharic Wikipedia\", \" Our results show that the proposed solution exhibits 70\", \"68{\\\\%} of precision, 66\", \"89{\\\\%} of recall and 68\", \"53{\\\\%} of f-measure in semantic annotation for a morphologically complex Amharic language with limited size dataset\", \"\"], \"labels\": [\"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\"]}\n",
      "In natural language one idea can be conveyed using different sentences; higher Natural Language Processing applications get difficulties in capturing meaning of ideas stated in different expressions. To solve this difficulty, different scholars have conducted Natural Language Inference (NLI) researches using methods from traditional discrete models with hard logic to an end-to-end neural network for different languages. In context of Amharic language, even though there are number of research efforts in higher NLP applications, still they have limitation on understanding idea expressed in different ways due to an absence of NLI in Amharic language. Accordingly, we proposed deep learning based Natural Language Inference using similarity and farness aware bidirectional attentive matching for Amharic texts. The experiment on limited Amharic NLI dataset prepared also shows promising result that can be used as baseline for subsequent works.\n",
      "{\"abstract_id\": 4, \"sentences\": [\"In natural language one idea can be conveyed using different sentences; higher Natural Language Processing applications get difficulties in capturing meaning of ideas stated in different expressions\", \" To solve this difficulty, different scholars have conducted Natural Language Inference (NLI) researches using methods from traditional discrete models with hard logic to an end-to-end neural network for different languages\", \" In context of Amharic language, even though there are number of research efforts in higher NLP applications, still they have limitation on understanding idea expressed in different ways due to an absence of NLI in Amharic language\", \" Accordingly, we proposed deep learning based Natural Language Inference using similarity and farness aware bidirectional attentive matching for Amharic texts\", \" The experiment on limited Amharic NLI dataset prepared also shows promising result that can be used as baseline for subsequent works\", \"\"], \"labels\": [\"0\", \"0\", \"0\", \"0\", \"0\", \"0\"]}\n",
      "Automatic Speech Recognition (ASR) is one of the most important technologies to help people live a better life in the 21st century. However, its development requires a big speech corpus for a language. The development of such a corpus is expensive especially for under-resourced Ethiopian languages. To address this problem we have developed four medium-sized (longer than 22 hours each) speech corpora for four Ethiopian languages: Amharic, Tigrigna, Oromo, and Wolaytta. In a way of checking the usability of the corpora and deliver a baseline ASR for each language. In this paper, we present the corpora and the baseline ASR systems for each language. The word error rates (WERs) we achieved show that the corpora are usable for further investigation and we recommend the collection of text corpora to train strong language models for Oromo and Wolaytta compared to others.\n",
      "{\"abstract_id\": 5, \"sentences\": [\"Automatic Speech Recognition (ASR) is one of the most important technologies to help people live a better life in the 21st century\", \" However, its development requires a big speech corpus for a language\", \" The development of such a corpus is expensive especially for under-resourced Ethiopian languages\", \" To address this problem we have developed four medium-sized (longer than 22 hours each) speech corpora for four Ethiopian languages: Amharic, Tigrigna, Oromo, and Wolaytta\", \" In a way of checking the usability of the corpora and deliver a baseline ASR for each language\", \" In this paper, we present the corpora and the baseline ASR systems for each language\", \" The word error rates (WERs) we achieved show that the corpora are usable for further investigation and we recommend the collection of text corpora to train strong language models for Oromo and Wolaytta compared to others\", \"\"], \"labels\": [\"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\"]}\n",
      "Most research on Lexical Simplification (LS) addresses non-native speakers of English, since they are numerous and easy to recruit. This makes it difficult to create LS solutions for other languages and target audiences. This paper presents SIMPLEX-PB 2.0, a dataset for LS in Brazilian Portuguese that, unlike its predecessor SIMPLEX-PB, accurately captures the needs of Brazilian underprivileged children. To create SIMPLEX-PB 2.0, we addressed all limitations of the old SIMPLEX-PB through multiple rounds of manual annotation. As a result, SIMPLEX-PB 2.0 features much more reliable and numerous candidate substitutions to complex words, as well as word complexity rankings produced by a group underprivileged children.\n",
      "{\"abstract_id\": 6, \"sentences\": [\"Most research on Lexical Simplification (LS) addresses non-native speakers of English, since they are numerous and easy to recruit\", \" This makes it difficult to create LS solutions for other languages and target audiences\", \" This paper presents SIMPLEX-PB 2\", \"0, a dataset for LS in Brazilian Portuguese that, unlike its predecessor SIMPLEX-PB, accurately captures the needs of Brazilian underprivileged children\", \" To create SIMPLEX-PB 2\", \"0, we addressed all limitations of the old SIMPLEX-PB through multiple rounds of manual annotation\", \" As a result, SIMPLEX-PB 2\", \"0 features much more reliable and numerous candidate substitutions to complex words, as well as word complexity rankings produced by a group underprivileged children\", \"\"], \"labels\": [\"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\"]}\n",
      "So far different research works have been conducted to achieve short answer questions. Hence, due to the advancement of artificial intelligence and adaptability of deep learning models, we introduced a new model to score short answer subjective questions. Using bi-directional answer to answer co-attention, we have demonstrated the extent to which each words and sentences features of student answer detected by the model and shown prom-ising result on both Kaggle and Mohler{'}s dataset. The experiment on Amharic short an-swer dataset prepared for this research work also shows promising result that can be used as baseline for subsequent works.\n",
      "{\"abstract_id\": 7, \"sentences\": [\"So far different research works have been conducted to achieve short answer questions\", \" Hence, due to the advancement of artificial intelligence and adaptability of deep learning models, we introduced a new model to score short answer subjective questions\", \" Using bi-directional answer to answer co-attention, we have demonstrated the extent to which each words and sentences features of student answer detected by the model and shown prom-ising result on both Kaggle and Mohler{'}s dataset\", \" The experiment on Amharic short an-swer dataset prepared for this research work also shows promising result that can be used as baseline for subsequent works\", \"\"], \"labels\": [\"0\", \"0\", \"0\", \"0\", \"0\"]}\n",
      "An interesting challenge for situated dialogue systems is referential visual dialog: by asking questions, the system has to identify the referent to which the user refers to. Task success is the standard metric used to evaluate these systems. However, it does not consider how effective each question is, that is how much each question contributes to the goal. We propose a new metric, that measures question effectiveness. As a preliminary study, we report the new metric for state of the art publicly available models on GuessWhat?!. Surprisingly, successful dialogues do not have a higher percentage of effective questions than failed dialogues. This suggests that a system with high task success is not necessarily one that generates good questions.\n",
      "{\"abstract_id\": 8, \"sentences\": [\"An interesting challenge for situated dialogue systems is referential visual dialog: by asking questions, the system has to identify the referent to which the user refers to\", \" Task success is the standard metric used to evaluate these systems\", \" However, it does not consider how effective each question is, that is how much each question contributes to the goal\", \" We propose a new metric, that measures question effectiveness\", \" As a preliminary study, we report the new metric for state of the art publicly available models on GuessWhat?!\", \" Surprisingly, successful dialogues do not have a higher percentage of effective questions than failed dialogues\", \" This suggests that a system with high task success is not necessarily one that generates good questions\", \"\"], \"labels\": [\"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\"]}\n",
      "{``}Low resource languages{''} usually refers to languages that lack corpora and basic tools such as part-of-speech taggers. But a significant number of such languages do benefit from the availability of relatively complex linguistic descriptions of phonology, morphology, and syntax, as well as dictionaries. A further category, probably the majority of the world{'}s languages, suffers from the lack of even these resources. In this paper, we investigate the possibility of learning the morphology of such a language by relying on its close relationship to a language with more resources. Specifically, we use a transfer-based approach to learn the morphology of the severely under-resourced language Gofa, starting with a neural morphological generator for the closely related language, Wolaytta. Both languages are members of the Omotic family, spoken and southwestern Ethiopia, and, like other Omotic languages, both are morphologically complex. We first create a finite- state transducer for morphological analysis and generation for Wolaytta, based on relatively complete linguistic descriptions and lexicons for the language. Next, we train an encoder-decoder neural network on the task of morphological generation for Wolaytta, using data generated by the FST. Such a network takes a root and a set of grammatical features as input and generates a word form as output. We then elicit Gofa translations of a small set of Wolaytta words from bilingual speakers. Finally, we retrain the decoder of the Wolaytta network, using a small set of Gofa target words that are translations of the Wolaytta outputs of the original network. The evaluation shows that the transfer network performs better than a separate encoder-decoder network trained on a larger set of Gofa words. We conclude with implications for the learning of morphology for severely under-resourced languages in regions where there are related languages with more resources.\n",
      "{\"abstract_id\": 9, \"sentences\": [\"{``}Low resource languages{''} usually refers to languages that lack corpora and basic tools such as part-of-speech taggers\", \" But a significant number of such languages do benefit from the availability of relatively complex linguistic descriptions of phonology, morphology, and syntax, as well as dictionaries\", \" A further category, probably the majority of the world{'}s languages, suffers from the lack of even these resources\", \" In this paper, we investigate the possibility of learning the morphology of such a language by relying on its close relationship to a language with more resources\", \" Specifically, we use a transfer-based approach to learn the morphology of the severely under-resourced language Gofa, starting with a neural morphological generator for the closely related language, Wolaytta\", \" Both languages are members of the Omotic family, spoken and southwestern Ethiopia, and, like other Omotic languages, both are morphologically complex\", \" We first create a finite- state transducer for morphological analysis and generation for Wolaytta, based on relatively complete linguistic descriptions and lexicons for the language\", \" Next, we train an encoder-decoder neural network on the task of morphological generation for Wolaytta, using data generated by the FST\", \" Such a network takes a root and a set of grammatical features as input and generates a word form as output\", \" We then elicit Gofa translations of a small set of Wolaytta words from bilingual speakers\", \" Finally, we retrain the decoder of the Wolaytta network, using a small set of Gofa target words that are translations of the Wolaytta outputs of the original network\", \" The evaluation shows that the transfer network performs better than a separate encoder-decoder network trained on a larger set of Gofa words\", \" We conclude with implications for the learning of morphology for severely under-resourced languages in regions where there are related languages with more resources\", \"\"], \"labels\": [\"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\"]}\n",
      "The Tigrinya language is agglutinative and has a large number of inflected and derived forms of words. Therefore a Tigrinya large vocabulary continuous speech recognition system often has a large number of different units and a high out-of-vocabulary (OOV) rate if a word is used as a recognition unit of a language model (LM) and lexicon. Therefore a morpheme-based approach has often been used and a morpheme is used as the recognition unit to reduce the high OOV rate. This paper presents an automatic speech recognition experiment conducted to see the effect of OOV words on the performance speech recognition system for Tigrinya. We tried to solve the OOV problem by using morphemes as lexicon and language model units. It has been found that the morpheme-based recognition system is better lexical and language modeling units than words. An absolute improvement (in word recognition accuracy) of 3.45 token and 8.36 types has been obtained as a result of using a morph-based vocabulary.\n",
      "{\"abstract_id\": 10, \"sentences\": [\"The Tigrinya language is agglutinative and has a large number of inflected and derived forms of words\", \" Therefore a Tigrinya large vocabulary continuous speech recognition system often has a large number of different units and a high out-of-vocabulary (OOV) rate if a word is used as a recognition unit of a language model (LM) and lexicon\", \" Therefore a morpheme-based approach has often been used and a morpheme is used as the recognition unit to reduce the high OOV rate\", \" This paper presents an automatic speech recognition experiment conducted to see the effect of OOV words on the performance speech recognition system for Tigrinya\", \" We tried to solve the OOV problem by using morphemes as lexicon and language model units\", \" It has been found that the morpheme-based recognition system is better lexical and language modeling units than words\", \" An absolute improvement (in word recognition accuracy) of 3\", \"45 token and 8\", \"36 types has been obtained as a result of using a morph-based vocabulary\", \"\"], \"labels\": [\"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\"]}\n"
     ]
    }
   ],
   "source": [
    "json_file = open('ann_abstract.jsonl', 'wb')\n",
    "\n",
    "index = 0\n",
    "for bib_id in bibdata.entries:\n",
    "    tmp_dict = {}\n",
    "    b = bibdata.entries[bib_id].fields\n",
    "    try:\n",
    "        ab = b['abstract']\n",
    "        print(ab)\n",
    "        tmp_dict['abstract_id'] = index\n",
    "        tmp_dict['sentences'] = ab.split('.')\n",
    "        tmp_dict['labels'] = ['0']*len(tmp_dict['sentences'])\n",
    "\n",
    "        json_str = json.dumps(tmp_dict)\n",
    "#         print(json_str)\n",
    "        json_file.write(json_str)\n",
    "    except:\n",
    "        pass\n",
    "    index += 1\n",
    "    if index > 10:\n",
    "        break\n",
    "json_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
